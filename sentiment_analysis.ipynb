{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Yuval\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load the Excel file\n",
    "file = 'posts100_tagged_with_GPT4.xlsx'\n",
    "sheets = pd.read_excel(file, sheet_name=None)  # Load all sheets\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Process each sheet\n",
    "for sheet_name, df in sheets.items():\n",
    "    # Calculate sentiment scores for each post and add them as new columns\n",
    "    df['neg'] = df['Post'].apply(lambda x: analyzer.polarity_scores(x)['neg'])\n",
    "    df['neu'] = df['Post'].apply(lambda x: analyzer.polarity_scores(x)['neu'])\n",
    "    df['pos'] = df['Post'].apply(lambda x: analyzer.polarity_scores(x)['pos'])\n",
    "    df['compound'] = df['Post'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "    \n",
    "    # Write back to the Excel file on separate sheets\n",
    "    with pd.ExcelWriter('posts100_tagged_with_GPT4_sentiment_analysis.xlsx', mode='w') as writer:\n",
    "        for name, sheet in sheets.items():\n",
    "            sheet.to_excel(writer, sheet_name=name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation between compound and answer1: 0.11801625995271506 in sport and art users posts\n",
      "Spearman correlation between compound and answer2: 0.06817226973549431 in sport and art users posts\n",
      "Spearman correlation between compound and answer1: 0.19504697292297227 in only art users posts\n",
      "Spearman correlation between compound and answer2: 0.11420540997741659 in only art users posts\n"
     ]
    }
   ],
   "source": [
    "# Define a function to encode the answers\n",
    "import pandas as pd\n",
    "\n",
    "def encode_answers(answer):\n",
    "    answer = answer.lower()\n",
    "    if answer in ['rather not hard to create', 'rather productive']:\n",
    "        return 1\n",
    "    elif answer in ['rather hard to create', 'rather not productive']:\n",
    "        return -1\n",
    "    elif answer in ['rather neutral to creation', 'rather neutral']:\n",
    "        return 0\n",
    "    elif answer == 'author asks for a relative or close friend only':\n",
    "        return None  # or some other encoding that makes sense for your analysis\n",
    "file = 'posts100_tagged_with_GPT4.xlsx'\n",
    "sheets = pd.read_excel(file, sheet_name=None)  # Load all sheets\n",
    "# Process each sheet\n",
    "for sheet_name, df in sheets.items():\n",
    "    # Assume df['answer1'] and df['answer2'] are columns with the answers\n",
    "    df['encoded_answer1'] = df['GPT Grade1'].apply(encode_answers)\n",
    "    df['encoded_answer2'] = df['GPT Grade2'].apply(encode_answers)\n",
    "    df['compound'] = df['Post'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "\n",
    "    # Calculate correlation and use Spearman because it's non-parametric and doesn't assume normality\n",
    "    correlation_answer1 = df['compound'].corr(df['encoded_answer1'], method='spearman')\n",
    "    correlation_answer2 = df['compound'].corr(df['encoded_answer2'], method='spearman')\n",
    "\n",
    "    print(f\"Spearman correlation between compound and answer1: {correlation_answer1} in {sheet_name}\")\n",
    "    print(f\"Spearman correlation between compound and answer2: {correlation_answer2} in {sheet_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage accuracy of matches 1: 52.00% in sport and art users posts\n",
      "Percentage accuracy of matches 2: 36.00% in sport and art users posts\n",
      "Percentage accuracy of matches 1: 68.00% in only art users posts\n",
      "Percentage accuracy of matches 2: 28.00% in only art users posts\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Encode answers as previously defined\n",
    "def encode_answers(answer):\n",
    "    answer = answer.lower()\n",
    "    if answer in ['rather not hard to create', 'rather productive']:\n",
    "        return 1\n",
    "    elif answer in ['rather hard to create', 'rather not productive']:\n",
    "        return -1\n",
    "    elif answer in ['rather neutral to creation', 'rather neutral']:\n",
    "        return 0\n",
    "    elif answer == 'author asks for a relative or close friend only':\n",
    "        return None\n",
    "\n",
    "# Define a function to classify compound based on defined thresholds\n",
    "def classify_compound(compound):\n",
    "    if compound is None:\n",
    "        return None\n",
    "    elif compound > 0.3:\n",
    "        return 1\n",
    "    elif compound < -0.3:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "file = 'posts100_tagged_with_GPT4.xlsx'\n",
    "sheets = pd.read_excel(file, sheet_name=None)  # Load all sheets\n",
    "# Process each sheet\n",
    "for sheet_name, df in sheets.items():\n",
    "    df['encoded_answer1'] = df['GPT Grade1'].apply(encode_answers)\n",
    "    df['encoded_answer2'] = df['GPT Grade2'].apply(encode_answers)\n",
    "    df['compound'] = df['Post'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "\n",
    "    # Classify compound scores\n",
    "    df['classified_compound'] = df['compound'].apply(classify_compound)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    df_dropna = df.dropna(subset=['classified_compound', 'encoded_answer1'])\n",
    "    matches = df_dropna['classified_compound'] == df_dropna['encoded_answer1']\n",
    "    accuracy = matches.sum() / len(df_dropna) * 100\n",
    "    print(f\"Percentage accuracy of matches 1: {accuracy:.2f}% in {sheet_name}\")\n",
    "\n",
    "    df_dropna = df.dropna(subset=['classified_compound', 'encoded_answer2'])\n",
    "    matches = df_dropna['classified_compound'] == df_dropna['encoded_answer2']\n",
    "    accuracy = matches.sum() / len(df_dropna) * 100\n",
    "    print(f\"Percentage accuracy of matches 2: {accuracy:.2f}% in {sheet_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
